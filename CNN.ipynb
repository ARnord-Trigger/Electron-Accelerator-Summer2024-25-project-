{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\anves\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class FC_Layer:\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, W=None, bias=None):\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.m_bias = 0\n",
    "        self.v_bias = 0\n",
    "        self.t = 0  # timestep\n",
    "        if W == None:\n",
    "            self.W = np.random.randn(in_dim, out_dim)/fac\n",
    "\n",
    "        else:\n",
    "            self.W = W\n",
    "\n",
    "        if bias == None:\n",
    "            self.bias = np.random.randn(out_dim)/fac\n",
    "\n",
    "        else:\n",
    "            self.bias = bias\n",
    "        \n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.z = np.matmul(X, self.W) + self.bias\n",
    "        return self.z\n",
    "\n",
    "    \n",
    "    def backprop(self, grad_previous):\n",
    "        t= self.X.shape[0]\n",
    "        self.grad_W = np.matmul((self.X.transpose()), grad_previous)/t\n",
    "        self.grad_bias = (grad_previous.sum(axis=0))/t\n",
    "        self.grad_a = np.matmul(grad_previous, self.W.transpose())\n",
    "        return self.grad_a\n",
    "\n",
    "\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        \n",
    "        self.W = self.W - (self.alpha*self.grad)\n",
    "        self.bias = self.bias - (self.alpha*self.grad_bias)\n",
    "            \n",
    "    def applying_adam(self):\n",
    "        \n",
    "        self.t += 1\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * self.grad\n",
    "        self.m_bias = self.beta1 * self.m_bias + (1 - self.beta1) * self.grad_bias\n",
    "\n",
    "        # Update biased second moment estimate\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * np.square(self.grad)\n",
    "        self.v_bias = self.beta2 * self.v_bias + (1 - self.beta2) * np.square(self.grad_bias)\n",
    "\n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_hat = self.m / (1 - np.power(self.beta1, self.t))\n",
    "        m_hat_bias = self.m_bias / (1 - np.power(self.beta1, self.t))\n",
    "\n",
    "        # Compute bias-corrected second moment estimate\n",
    "        v_hat = self.v / (1 - np.power(self.beta2, self.t))\n",
    "        v_hat_bias = self.v_bias / (1 - np.power(self.beta2, self.t))\n",
    "\n",
    "        # Update parameters\n",
    "        self.W -= self.alpha * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        self.bias -= self.alpha * m_hat_bias / (np.sqrt(v_hat_bias) + self.epsilon)           \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
