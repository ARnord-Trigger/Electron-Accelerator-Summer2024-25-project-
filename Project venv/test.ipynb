{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Output:\n",
      "[[ -5.65184052   2.02919721  -0.10014337]\n",
      " [ -9.02948585   3.38299445  -0.40440482]\n",
      " [-12.40713119   4.73679169  -0.70866626]\n",
      " [-15.78477652   6.09058893  -1.01292771]]\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "Cannot cast ufunc 'add' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(forward_output)\n\u001b[0;32m     80\u001b[0m doutput \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m*\u001b[39mforward_output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 81\u001b[0m backward_output \u001b[38;5;241m=\u001b[39m \u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackward Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(backward_output)\n",
      "Cell \u001b[1;32mIn[2], line 71\u001b[0m, in \u001b[0;36mCNN.backward\u001b[1;34m(self, doutput)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, doutput):\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m, in \u001b[0;36mConv1D.backward\u001b[1;34m(self, doutput)\u001b[0m\n\u001b[0;32m     54\u001b[0m         dfilters[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m doutput[i, j] \u001b[38;5;241m*\u001b[39m segment\n\u001b[0;32m     55\u001b[0m         dbiases[j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m doutput[i, j]\n\u001b[1;32m---> 56\u001b[0m         \u001b[43mdinput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdoutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiases\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases}, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilters\u001b[39m\u001b[38;5;124m'\u001b[39m: dfilters, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiases\u001b[39m\u001b[38;5;124m'\u001b[39m: dbiases})\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dinput\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: Cannot cast ufunc 'add' output from dtype('float64') to dtype('int32') with casting rule 'same_kind'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdamOptimizer:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "        self.m = {}\n",
    "        self.v = {}\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.t += 1\n",
    "        for key in params.keys():\n",
    "            if key not in self.m:\n",
    "                self.m[key] = np.zeros_like(grads[key])\n",
    "                self.v[key] = np.zeros_like(grads[key])\n",
    "\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "class Conv1D:\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        self.filters = np.random.randn(num_filters, filter_size) / np.sqrt(num_filters / 2)\n",
    "        self.biases = np.zeros(num_filters)\n",
    "        self.optimizer = AdamOptimizer()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        self.output = np.zeros((input.shape[0] - self.filter_size + 1, self.num_filters))\n",
    "\n",
    "        for i in range(self.output.shape[0]):\n",
    "            segment = input[i:i+self.filter_size]\n",
    "            self.output[i] = np.sum(segment * self.filters, axis=1) + self.biases\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, doutput):\n",
    "        dfilters = np.zeros_like(self.filters)\n",
    "        dbiases = np.zeros_like(self.biases)\n",
    "        dinput = np.zeros_like(self.input)\n",
    "\n",
    "        for i in range(doutput.shape[0]):\n",
    "            segment = self.input[i:i+self.filter_size]\n",
    "            for j in range(self.num_filters):\n",
    "                dfilters[j] += doutput[i, j] * segment\n",
    "                dbiases[j] += doutput[i, j]\n",
    "                dinput[i:i+self.filter_size] += doutput[i, j] * self.filters[j]\n",
    "\n",
    "        self.optimizer.update({'filters': self.filters, 'biases': self.biases}, {'filters': dfilters, 'biases': dbiases})\n",
    "\n",
    "        return dinput\n",
    "\n",
    "class CNN:\n",
    "    def __init__(self, num_filters, filter_size):\n",
    "        self.conv1d = Conv1D(num_filters, filter_size)\n",
    "        self.optimizer = AdamOptimizer()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.conv1d.forward(input)\n",
    "\n",
    "    def backward(self, doutput):\n",
    "        return self.conv1d.backward(doutput)\n",
    "\n",
    "# Example usage:\n",
    "cnn = CNN(num_filters=3, filter_size=2)\n",
    "input = np.array([1, 2, 3, 4, 5])\n",
    "forward_output = cnn.forward(input)\n",
    "print(\"Forward Output:\")\n",
    "print(forward_output)\n",
    "\n",
    "doutput = np.random.randn(*forward_output.shape)\n",
    "backward_output = cnn.backward(doutput)\n",
    "print(\"Backward Output:\")\n",
    "print(backward_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array() got an unexpected keyword argument 'keepdims'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m a\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mTypeError\u001b[0m: array() got an unexpected keyword argument 'keepdims'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 \t batch: 1 \t loss: \t 5.070933240127142\taccuracy: 6.416666666666666\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'relu' object has no attribute 'a_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 363\u001b[0m\n\u001b[0;32m    361\u001b[0m     broke \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[43mcomplete_NN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_testing\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43mii\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m complete_NN\u001b[38;5;241m.\u001b[39mapplying_sgd()\n\u001b[0;32m    365\u001b[0m k \u001b[38;5;241m=\u001b[39m ii\n",
      "Cell \u001b[1;32mIn[8], line 254\u001b[0m, in \u001b[0;36mNeural_Network.backprop\u001b[1;34m(self, Y)\u001b[0m\n\u001b[0;32m    252\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mNetwork)):\n\u001b[1;32m--> 254\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 111\u001b[0m, in \u001b[0;36mrelu.backprop\u001b[1;34m(self, grad_previous)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(p):\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m iii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(t):\n\u001b[1;32m--> 111\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad[i, ii, iii] \u001b[38;5;241m=\u001b[39m (grad_previous[i, ii, iii] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivative(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma_1\u001b[49m[i, ii, iii]))\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'relu' object has no attribute 'a_1'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "fac = 5\n",
    "Mnist = tf.keras.datasets.mnist\n",
    "\n",
    "class Linear_Layer:\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, alpha = 0.01, Theta = None, bias = None):\n",
    "        self.alpha = alpha\n",
    "        if Theta == None:\n",
    "            self.Theta = np.random.randn(in_dim, out_dim)/fac\n",
    "\n",
    "        else:\n",
    "            self.Theta = Theta\n",
    "\n",
    "        if bias == None:\n",
    "            self.bias = np.random.randn(out_dim)/fac\n",
    "\n",
    "        else:\n",
    "            self.bias = bias\n",
    "        \n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.z = np.matmul(X, self.Theta) + self.bias\n",
    "        return self.z\n",
    "\n",
    "    \n",
    "    def backprop(self, grad_previous):\n",
    "        t= self.X.shape[0]\n",
    "        self.grad = np.matmul((self.X.transpose()), grad_previous)/t\n",
    "        self.grad_bias = (grad_previous.sum(axis=0))/t\n",
    "        self.grad_a = np.matmul(grad_previous, self.Theta.transpose())\n",
    "        return self.grad_a\n",
    "\n",
    "\n",
    "\n",
    "    def applying_sgd(self):\n",
    "            self.Theta = self.Theta - (self.alpha*self.grad)\n",
    "            self.bias = self.bias - (self.alpha*self.grad_bias)\n",
    "\n",
    "class softmax:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def expansion(self, t):\n",
    "        (a,) = t.shape\n",
    "        Y = np.zeros((a,10))\n",
    "        for i in range(0,a):\n",
    "            Y[i,t[i]] = 1\n",
    "        return Y\n",
    "    \n",
    "    def forward_pass(self, z):\n",
    "        self.z =  z\n",
    "        (p,t) = self.z.shape\n",
    "        self.a = np.zeros((p,t))\n",
    "        for i in range(0,p):\n",
    "            for ii in range(0,t):\n",
    "                self.a[i,ii] = (np.exp(self.z[i,ii]))/(np.sum(np.exp(self.z[i,:])))\n",
    "        return self.a\n",
    "\n",
    "    def backprop(self, Y):\n",
    "        y = self.expansion(Y)\n",
    "        self.grad = (self.a - y)\n",
    "        return self.grad\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        pass\n",
    "\n",
    "class relu:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward_pass(self, z):\n",
    "        \n",
    "        # if (len(z.shape) == 3):\n",
    "\n",
    "        #     z_temp = z.reshape((z.shape[0], z.shape[1]*z.shape[2]))\n",
    "        #     z_temp_1 = self.forward_pass(z_temp)\n",
    "        #     self.a_1 = z_temp_1.reshape((z.shape[0], z.shape[1], z.shape[2]))\n",
    "        #     return (self.a_1)\n",
    "\n",
    "        # else:\n",
    "        #     (p,t) = z.shape\n",
    "        #     self.a = np.zeros((p,t))\n",
    "        #     for i in range(0,p):\n",
    "        #         for ii in range(0,t):\n",
    "        #                 self.a[i,ii] = max([0,z[i,ii]])\n",
    "        #     return self.a\n",
    "        return np.maximum(0,z)\n",
    "\n",
    "    def derivative(self, a):\n",
    "        if a>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def backprop(self, grad_previous):\n",
    "        \n",
    "        if (len(grad_previous.shape)==3):\n",
    "\n",
    "            (d, p, t) = grad_previous.shape\n",
    "            self.grad = np.zeros((d, p, t))\n",
    "            \n",
    "            for i in range(d):\n",
    "                for ii in range(p):\n",
    "                    for iii in range(t):\n",
    "                        self.grad[i, ii, iii] = (grad_previous[i, ii, iii] * self.derivative(self.a_1[i, ii, iii]))\n",
    "            \n",
    "            return (self.grad)\n",
    "\n",
    "        else:\n",
    "            (p,t) = grad_previous.shape\n",
    "            self.grad = np.zeros((p,t))\n",
    "            for i in range(p):\n",
    "                for ii in range(t):\n",
    "                    self.grad[i,ii] = grad_previous[i,ii] * self.derivative(self.a[i,ii])\n",
    "            return (self.grad)\n",
    "\n",
    "    \n",
    "    def applying_sgd(self):\n",
    "        pass\n",
    "\n",
    "class padding():\n",
    "    \n",
    "    def __init__(self, pad = 1):\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward_pass(self, data):\n",
    "        X = np.pad(data , ((0, 0), (self.pad, self.pad), (self.pad, self.pad)),'constant', constant_values=0)\n",
    "        return X\n",
    "\n",
    "    def backprop(self, y):\n",
    "        return (y[:, 1:(y.shape[1]-1),1:(y.shape[2]-1)])\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        pass\n",
    "\n",
    "class Convolutional_Layer:\n",
    "\n",
    "    def __init__(self, filter_dim = 3, stride = 1, pad = 1, alpha=0.01):\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.filter = np.random.randn(self.filter_dim, self.filter_dim)\n",
    "        self.filter = self.filter/self.filter.sum()\n",
    "        self.bias = np.random.rand()/10\n",
    "        self.pad = pad\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def convolving(self, X, fil, dimen_x, dimen_y):\n",
    "        z = np.zeros((dimen_x, dimen_y))\n",
    "        for i in range(dimen_x):\n",
    "            for ii in range(dimen_y):\n",
    "                temp = np.multiply(X[i : i+fil.shape[0], ii : ii+fil.shape[1]], fil)\n",
    "                z[i,ii] = temp.sum()\n",
    "        return z\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        (d, p, t) = self.X.shape\n",
    "        dimen_x = int(((p - self.filter_dim)/self.stride) + 1)\n",
    "        dimen_y = int(((t - self.filter_dim)/self.stride) + 1)\n",
    "        self.z = np.zeros((d, dimen_x, dimen_y))\n",
    "        for i in range(d):\n",
    "            self.z[i] = (self.convolving(self.X[i], self.filter, dimen_x, dimen_y) + self.bias)\n",
    "\n",
    "        return self.z\n",
    "\n",
    "    def backprop(self, grad_z):\n",
    "        (d, p, t) = grad_z.shape\n",
    "        filter_1 = np.flip((np.flip(self.filter, axis = 0)), axis = 1)\n",
    "        self.grads = np.zeros((d, p, t))\n",
    "        for i in range(d):\n",
    "            self.grads[i] = self.convolving(np.pad(grad_z[i], ((1,1), (1,1)), 'constant', constant_values = 0), filter_1, p, t)\n",
    "\n",
    "        self.grads = np.pad(self.grads, ((0,0),(1,1),(1,1)), 'constant', constant_values = 0)\n",
    "\n",
    "        self.grad_filter = np.zeros((self.filter_dim, self.filter_dim))\n",
    "\n",
    "        for i in range(self.filter_dim):\n",
    "            for ii in range(self.filter_dim):\n",
    "                self.grad_filter[i, ii] = (np.multiply(grad_z, self.X[:, i:p+i, ii:t+ii])).sum()\n",
    "        self.grad_filter = self.grad_filter/(d)\n",
    "\n",
    "        self.grad_bias = (grad_z.sum())/(d)\n",
    "        return self.grads\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        self.filter = self.filter - (self.alpha*self.grad_filter)\n",
    "        self.bias = self.bias - (self.alpha*self.grad_bias)\n",
    "\n",
    "\n",
    "class Max_pooling:\n",
    "\n",
    "    def __init__(self, pool_dim = 2, stride = 2):\n",
    "        self.pool_dim = pool_dim\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward_pass(self, data):\n",
    "        (q, p, t) = data.shape\n",
    "        z_x = int((p - self.pool_dim) / self.stride) + 1\n",
    "        z_y = int((t - self.pool_dim) / self.stride) + 1\n",
    "        after_pool = np.zeros((q, z_x, z_y))\n",
    "        for ii in range(0, q):\n",
    "            liss = []\n",
    "            for i in range(0,p,self.stride):\n",
    "                for j in range(0,t,self.stride):\n",
    "                    if (i+self.pool_dim <= p) and (j+self.pool_dim <= t):\n",
    "                        temp = data[ii, i:(i+(self.pool_dim)), j:(j+(self.pool_dim))]\n",
    "                        temp_1 = np.max(temp)\n",
    "                        liss.append(temp_1)\n",
    "            liss = np.asarray(liss)\n",
    "            liss = liss.reshape((z_x, z_y))\n",
    "            after_pool[ii] = liss\n",
    "            del liss\n",
    "        return after_pool\n",
    "\n",
    "    def backprop(self, pooled):\n",
    "        (a,b,c) = pooled.shape   \n",
    "        cheated = np.zeros((a,2*b,2*c))\n",
    "        for k in range(0, a):\n",
    "            pooled_transpose_re = pooled[k].reshape((b*c))\n",
    "            count = 0\n",
    "            for i in range(0, 2*b, self.stride):\n",
    "                for j in range(0, 2*c, self.stride):\n",
    "                    cheated[k, i:(i+(self.stride)),j:(j+(self.stride))] = pooled_transpose_re[count]\n",
    "                    count = count+1\n",
    "        return cheated\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Neural_Network:\n",
    "\n",
    "    def __init__(self, Network):\n",
    "        self.Network = Network\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        n = X\n",
    "        for i in self.Network:\n",
    "            n = i.forward_pass(n)\n",
    "            \n",
    "            \n",
    "        return n\n",
    "    \n",
    "    def backprop(self, Y):\n",
    "        m = Y\n",
    "        count = 1\n",
    "        for i in (reversed(self.Network)):\n",
    "            m = i.backprop(m)\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        for i in self.Network:\n",
    "            i.applying_sgd()\n",
    "\n",
    "\n",
    "class reshaping:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward_pass(self, a):\n",
    "        self.shape_a = a.shape\n",
    "        \n",
    "        self.final_a = a.reshape(self.shape_a[0], self.shape_a[1]*self.shape_a[2])\n",
    "        return self.final_a\n",
    "    \n",
    "    def backprop(self, q):\n",
    "        return (q.reshape(self.shape_a[0], self.shape_a[1], self.shape_a[2]))\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class cross_entropy:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def expansion(self, t):\n",
    "        (a,) = t.shape\n",
    "        Y = np.zeros((a,10))\n",
    "        for i in range(0,a):\n",
    "            Y[i,t[i]] = 1\n",
    "        return Y\n",
    "\n",
    "    def loss(self, A, Y):\n",
    "        exp_Y = self.expansion(Y)\n",
    "        (u,i) = A.shape\n",
    "        loss_matrix = np.zeros((u,i))\n",
    "        for j in range(u):\n",
    "            for jj in range(i):\n",
    "                if exp_Y[j,jj] == 0:\n",
    "                    loss_matrix[j,jj] = np.log(1 - A[j,jj])\n",
    "                else:\n",
    "                    loss_matrix[j,jj] = np.log(A[j,jj])\n",
    "        \n",
    "\n",
    "        return ((-(loss_matrix.sum()))/u)\n",
    "\n",
    "class accuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def value(self, out, Y):\n",
    "        self.out = np.argmax(out, axis=1)\n",
    "        p = self.out.shape[0]\n",
    "        total = 0\n",
    "        for i in range(p):\n",
    "            if Y[i]==self.out[i]:\n",
    "                total += 1\n",
    "        return total/p\n",
    "\n",
    "\n",
    "\n",
    "(Xtr, Ytr), (Xte, Yte) = Mnist.load_data()\n",
    "X_testing = Xtr[:,:,:]\n",
    "Y_testing = Ytr[:]\n",
    "#X_testing = X_testing.reshape((60000, 28*28))\n",
    "X_testing = X_testing/255\n",
    "al = 0.3\n",
    "stopper = 85.0\n",
    "\n",
    "complete_NN = Neural_Network([\n",
    "                                \n",
    "                                padding(),\n",
    "                                Convolutional_Layer(),\n",
    "                                Max_pooling(),\n",
    "                                relu(),\n",
    "                                padding(),\n",
    "                                Convolutional_Layer(),\n",
    "                                Max_pooling(),\n",
    "                                relu(),\n",
    "                                reshaping(),\n",
    "                                Linear_Layer(7*7, 24, alpha = al),\n",
    "                                relu(),\n",
    "                                Linear_Layer(24, 10, alpha = al),\n",
    "                                softmax()\n",
    "\n",
    "                                ])\n",
    "CE = cross_entropy()\n",
    "\n",
    "acc = accuracy()\n",
    "epochs = 100\n",
    "broke = 0\n",
    "batches = 6000\n",
    "for i in range(epochs):\n",
    "    k = 0\n",
    "    for ii in range(batches, 60001, batches):\n",
    "        \n",
    "        out = complete_NN.forward_pass(X_testing[k:ii])\n",
    "        print(\"epoch:{} \\t batch: {} \\t loss: \\t {}\".format(i+1, int(ii/batches), CE.loss(out, Y_testing[k:ii])), end=\"\\t\")\n",
    "        accur = acc.value(out, Y_testing[k:ii])*100\n",
    "        print(\"accuracy: {}\".format(accur))\n",
    "        \n",
    "        if accur >= stopper:\n",
    "            broke = 1\n",
    "            break\n",
    "        complete_NN.backprop(Y_testing[k:ii])\n",
    "        complete_NN.applying_sgd()\n",
    "        k = ii\n",
    "        \n",
    "    if broke == 1:\n",
    "        break\n",
    "    \n",
    "\n",
    "out = complete_NN.forward_pass(X_testing)\n",
    "print(\"The final loss is {}\".format(CE.loss(out, Y_testing)))\n",
    "print(\"The final accuracy on train set is {}\".format(acc.value(out, Y_testing)*100))\n",
    "Xtest = Xte/255\n",
    "#Xtest = Xte.reshape((10000,28*28))/255\n",
    "out_1 = complete_NN.forward_pass(Xtest)\n",
    "print(\"The accuracy on test set is {}\".format(acc.value(out_1, Yte)*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 4]\n",
      "  [0 2 1]]\n",
      "\n",
      " [[1 0 4]\n",
      "  [0 2 1]]\n",
      "\n",
      " [[1 0 4]\n",
      "  [0 2 1]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "b = np.array([[[1,-23,4],[0,2,1]],\n",
    "              [[1,-23,4],[0,2,1]],\n",
    "              [[1,-23,4],[0,2,1]]])\n",
    "b.shape\n",
    "print(np.maximum(0,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 0]\n",
      "  [0 1 2 0]\n",
      "  [0 3 4 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 2 0]\n",
      "  [0 3 4 0]\n",
      "  [0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0]\n",
      "  [0 1 2 0]\n",
      "  [0 3 4 0]\n",
      "  [0 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "grads = np.array([[[1,2],[3,4]],[[1,2],[3,4]],[[1,2],[3,4]]])\n",
    "grads = np.pad(grads, ((0,0),(1,1),(1,1)), 'constant', constant_values = 0)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
