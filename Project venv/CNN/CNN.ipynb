{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "class FC_Layer:\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, W=None, bias=None):\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.m_bias = 0\n",
    "        self.v_bias = 0\n",
    "        self.t = 0  # timestep\n",
    "        if W == None:\n",
    "            self.W = np.random.randn(in_dim, out_dim)/fac\n",
    "\n",
    "        else:\n",
    "            self.W = W\n",
    "\n",
    "        if bias == None:\n",
    "            self.bias = np.random.randn(out_dim)/fac\n",
    "\n",
    "        else:\n",
    "            self.bias = bias\n",
    "        \n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.X = X\n",
    "        self.z = np.matmul(X, self.W) + self.bias\n",
    "        return self.z\n",
    "\n",
    "    \n",
    "    def backprop(self, grad_previous):\n",
    "        t= self.X.shape[0]\n",
    "        self.grad_W = np.matmul((self.X.transpose()), grad_previous)/t\n",
    "        self.grad_bias = (grad_previous.sum(axis=0))/t\n",
    "        self.grad_a = np.matmul(grad_previous, self.W.transpose())\n",
    "        return self.grad_a\n",
    "\n",
    "\n",
    "\n",
    "    def applying_sgd(self):\n",
    "        \n",
    "        self.W = self.W - (self.alpha*self.grad)\n",
    "        self.bias = self.bias - (self.alpha*self.grad_bias)\n",
    "            \n",
    "    def applying_adam(self):\n",
    "        \n",
    "        self.t += 1\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * self.grad\n",
    "        self.m_bias = self.beta1 * self.m_bias + (1 - self.beta1) * self.grad_bias\n",
    "\n",
    "        # Update biased second moment estimate\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * np.square(self.grad)\n",
    "        self.v_bias = self.beta2 * self.v_bias + (1 - self.beta2) * np.square(self.grad_bias)\n",
    "\n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_hat = self.m / (1 - np.power(self.beta1, self.t))\n",
    "        m_hat_bias = self.m_bias / (1 - np.power(self.beta1, self.t))\n",
    "\n",
    "        # Compute bias-corrected second moment estimate\n",
    "        v_hat = self.v / (1 - np.power(self.beta2, self.t))\n",
    "        v_hat_bias = self.v_bias / (1 - np.power(self.beta2, self.t))\n",
    "\n",
    "        # Update parameters\n",
    "        self.W -= self.alpha * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        self.bias -= self.alpha * m_hat_bias / (np.sqrt(v_hat_bias) + self.epsilon)           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "         \n",
    "    \n",
    "    def forward_pass(self ,z):\n",
    "        self.z = z \n",
    "        self.a = np.zeros(self.z.shape)\n",
    "        self.a = np.exp(z)/np.sum(np.exp(z),axis =1 , keepdims = True )\n",
    "        return self.a\n",
    "    \n",
    "    def expansion(self):pass\n",
    "    \n",
    "    def applying_sgd(self):pass\n",
    "    def applying_adam(self):pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward_pass(self ,z):\n",
    "        self.a = np.maximum(0,z)\n",
    "        return self.a\n",
    "    def derivative(self, a):\n",
    "        if a>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def backprop(self, grad_previous):\n",
    "        \n",
    "        if (len(grad_previous.shape)==3):\n",
    "\n",
    "            (d, p, t) = grad_previous.shape\n",
    "            self.grad = np.zeros((d, p, t))\n",
    "            \n",
    "            for i in range(d):\n",
    "                for ii in range(p):\n",
    "                    for iii in range(t):\n",
    "                        self.grad[i, ii, iii] = (grad_previous[i, ii, iii] * self.derivative(self.a_1[i, ii, iii]))\n",
    "            \n",
    "            return (self.grad)\n",
    "\n",
    "        else:\n",
    "            (p,t) = grad_previous.shape\n",
    "            self.grad = np.zeros((p,t))\n",
    "            for i in range(p):\n",
    "                for ii in range(t):\n",
    "                    self.grad[i,ii] = grad_previous[i,ii] * self.derivative(self.a[i,ii])\n",
    "            return (self.grad)    \n",
    "    \n",
    "    def applying_sgd(self):pass\n",
    "    def applying_adam(self):pass   \n",
    "        \n",
    "\n",
    "# def forward_pass(z):\n",
    "#     a = np.maximum(0,z)\n",
    "#     return a    \n",
    "# # b = np.array([[[1,-23,-4],[0,2,1]],\n",
    "# #               [[1,-23,4],[0,2,1]],\n",
    "# #               [[1,-23,4],[0,2,1]]])\n",
    "# # print(forward_pass(b))\n",
    "# def derivative(self, a):\n",
    "#         if a>0:\n",
    "#             return 1\n",
    "#         else:\n",
    "#             return 0\n",
    "        \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class padding():\n",
    "    \n",
    "    def __init__(self, pad = 1):\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward_pass(self, data):\n",
    "        X = np.pad(data , ((0, 0), (self.pad, self.pad), (self.pad, self.pad)),'constant', constant_values=0)\n",
    "        return X\n",
    "\n",
    "    def backprop(self, y):\n",
    "        return (y[:, 1:(y.shape[1]-1),1:(y.shape[2]-1)])\n",
    "\n",
    "    def applying_sgd(self):pass\n",
    "    def applying_adam(self):pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_Layer:\n",
    "    def __init__(self ,filter_dim = 3, stride=1 , pad = 1, alpha = 0.001):\n",
    "        self.filter_dim = filter_dim\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        self.alpha = alpha \n",
    "        self.filter = np.random.randn(self.filter_dim , self.filter_dim)\n",
    "        self.filter = self.filter/self.filter.sum()\n",
    "        self.bias = np.random.randn()\n",
    "        self.bias = self.bias/self.bias.sum()\n",
    "    \n",
    "    def conv(self , X , fil , dimen_x , dimen_y):\n",
    "        z = np.zeros((dimen_x , dimen_y))\n",
    "        for i in range(dimen_x):\n",
    "            for j in range(dimen_y):\n",
    "                c = np.multiply(X[i:i+fil.shape[0] , j:j+fil.shape[1]] , fil)    \n",
    "                z[i,j] = c.sum()\n",
    "        return z\n",
    "    \n",
    "    def forward_pass(self ,X):\n",
    "        self.X = X\n",
    "        (d, p, t) = self.X.shape\n",
    "        dimen_x = int(((p - self.filter_dim)/self.stride) + 1)\n",
    "        dimen_y = int(((t - self.filter_dim)/self.stride) + 1)\n",
    "        self.z = np.zeros((d, dimen_x, dimen_y))\n",
    "        for i in range(d):\n",
    "            self.z[i] = (self.convolving(self.X[i], self.filter, dimen_x, dimen_y) + self.bias)\n",
    "\n",
    "        return self.z\n",
    "    def backprop(self, grad_z):\n",
    "        (d, p, t) = grad_z.shape\n",
    "        filter_1 = np.flip((np.flip(self.filter, axis = 0)), axis = 1) #180 degree rtated filter\n",
    "        self.grads = np.zeros((d, p, t))\n",
    "        \n",
    "        # delL/delA --> to be passed in the prev layer\n",
    "        for i in range(d):\n",
    "            self.grads[i] = self.convolving(np.pad(grad_z[i], ((1,1), (1,1)), 'constant', constant_values = 0), filter_1, p, t)\n",
    "        self.grads = np.pad(self.grads, ((0,0),(1,1),(1,1)), 'constant', constant_values = 0)\n",
    "\n",
    "        #delL/delFilter --> to be used in optimizer\n",
    "        self.grad_filter = np.zeros((self.filter_dim, self.filter_dim))\n",
    "        for i in range(self.filter_dim):\n",
    "            for ii in range(self.filter_dim):\n",
    "                self.grad_filter[i, ii] = (np.multiply(grad_z, self.X[:, i:p+i, ii:t+ii])).sum()\n",
    "        self.grad_filter = self.grad_filter/(d)\n",
    "\n",
    "        #delL/delB --> to be used in optimizer\n",
    "        self.grad_bias = (grad_z.sum())/(d)\n",
    "        return self.grads\n",
    "    \n",
    "    def applying_sgd(self):\n",
    "        self.filter = self.filter - (self.alpha*self.grad_filter)\n",
    "        self.bias = self.bias - (self.alpha*self.grad_bias)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN :\n",
    "    def __init__(self , Network ,optimizer=\"sgd\"):\n",
    "        self.optim = optimizer\n",
    "        self.Network = Network\n",
    "        \n",
    "    def forward_pass(self , X):\n",
    "        n = X\n",
    "        for i in self.Network:\n",
    "            n=i.forward_pass(n) \n",
    "        return n\n",
    "    def backprop(self , Y):\n",
    "        m = Y\n",
    "        count = 1\n",
    "        for i in (reversed(self.Network)):\n",
    "            m = i.backprop(m)\n",
    "    def applying_optim(self):\n",
    "                \n",
    "        if(self.optim==\"sgd\"):\n",
    "            for i in self.Network:\n",
    "                i.applying_sgd()\n",
    "        \n",
    "        if(self.optim==\"adam\"):\n",
    "            for i in self.Network:\n",
    "                i.applying_adam()        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyQt5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
